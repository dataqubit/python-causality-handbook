{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - The Unreasonable Effectiveness of Linear Regression\n",
    "\n",
    "\n",
    "## All You Need is Regression\n",
    "\n",
    "When dealing with causal inference, we saw how there are two potential outcomes for each individual: $Y_0$ is the outcome the individual would have if he or she didn't take the treatment and $Y_1$ is the outcome if he or she took the treatment. The act of setting the treatment $T$ to 0 or 1 materializes one of the potential outcomes and makes it impossible for us to ever know the other one. This leads to the fact that the individual treatment effect $\\tau_i = Y_{1i} - Y_{0i}$ is unknowable. \n",
    "\n",
    "$\n",
    "Y_i = Y_{0i} + T_i(Y_{1i} - Y_{0i}) = Y_{0i}(1-T_i) + T_i Y_{1i}\n",
    "$\n",
    "\n",
    "So, for now, let's focus on the simpler task of estimating the average causal effect. With this in mind, we are accepting the fact that some people respond better than others to the treatment, but we are also accepting that we can't know who they are. Instead, we will just try to see if the treatment works, **on average**. \n",
    "\n",
    "$\n",
    "ATE = E[Y_1 - Y_0]\n",
    "$\n",
    "\n",
    "This will give us a simplified model, with a constant treatment effect $Y_{1i} = Y_{0i} + \\kappa$. If $\\kappa$ is positive, we will say that the treatment has, on average, a positive effect. Even if some people will respond badly to it, on average, the impact will be positive.\n",
    "\n",
    "Let's also recall that we can't simply estimate $E[Y_1 - Y_0]$ with the difference in mean $E[Y|T=1] - E[Y|T=0]$ due to bias. Bias often arises when the treated and untreated are different for reasons other than the treatment itself. One way to see this is on how they differ in the potential outcome $Y_0$\n",
    "\n",
    "$\n",
    "E[Y|T=1] - E[Y|T=0] = \\underbrace{E[Y_1 - Y_0|T=1]}_{ATET} + \\underbrace{\\{ E[Y_0|T=1] - E[Y_0|T=0]\\}}_{BIAS}\n",
    "$\n",
    "\n",
    "Previously, we saw how we can eliminate bias with Random Experiments, or **Randomised Controlled Trial** (RCT) as they are sometimes called. RCT forces the treated and the untreated to be equal and that's why the bias vanishes. We also saw how to place uncertainty levels around our estimates for the treatment effect. Namely, we looked at the case of online versus face-to-face classrooms, where $T=0$ represent face-to-face lectures and $T=1$ represent online ones. Students were randomly assigned to one of those 2 types of lectures and then their performance on an exam was evaluated. We've built an A/B testing function that could compare both groups, provide the average treatment effect and even place a confidence interval around it.\n",
    "\n",
    "Now, it's time to see that we can do all of that with the workhorse of causal inference: **Linear Regression**! Think of it this way. If comparing treated and untreated means was an apple for dessert, linear regression would be cold and creamy tiramisu. Or if comparing treated and untreated is a sad and old loaf of white wonder bread, linear regression would be a crusty, soft crumb country loaf sourdough baked by Chad Robertson himself.\n",
    "\n",
    "![img](./data/img/linear-regression/you_vs.png)\n",
    "\n",
    "Lets see how this beauty works. In the code below, we want to run the exact same analysis of comparing online vs face-to-face classes. But instead of doing all that math of confidence intervals, we just run a regression. More specifically, we estimate the following model:\n",
    "\n",
    "$\n",
    "exam_i = \\beta_0 + \\kappa \\ Online_i + u_i\n",
    "$\n",
    "\n",
    "This means we are modeling the exam outcome as a baseline $\\beta_0$ plus $\\kappa$ if the class is online.  Of course the exam result is driven by additional variables (like student's mood on the exam day, hours studied and so on). But we don't really care about understanding those relationships. So, instead, we use that $u_i$ term to represent everything else we don't care about. This is called the model error. \n",
    "\n",
    "Notice that $Online$ is our treatment indication and hence, a dummy variable. It is zero when the treatment is face to face and 1 if it's online. With that in mind, we can see that linear regression will recover $E[Y|T=0] = \\beta_0$ and $E[Y|T=1] = \\beta_0 + \\kappa $. $\\kappa$ will be our ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m637.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import graphviz as gr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   78.5475</td> <td>    1.113</td> <td>   70.563</td> <td> 0.000</td> <td>   76.353</td> <td>   80.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>format_ol</th> <td>   -4.9122</td> <td>    1.680</td> <td>   -2.925</td> <td> 0.004</td> <td>   -8.223</td> <td>   -1.601</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}  &      78.5475  &        1.113     &    70.563  &         0.000        &       76.353    &       80.742     \\\\\n",
       "\\textbf{format\\_ol} &      -4.9122  &        1.680     &    -2.925  &         0.004        &       -8.223    &       -1.601     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/online_classroom.csv\").query(\"format_blended==0\")\n",
    "\n",
    "result = smf.ols('falsexam ~ format_ol', data=data).fit()\n",
    "result.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Insert by dataqubit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hawaiian</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>unknown</th>\n",
       "      <th>white</th>\n",
       "      <th>format_ol</th>\n",
       "      <th>format_blended</th>\n",
       "      <th>falsexam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.29997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.96000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.34996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.99000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.05000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.69000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.29997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender  asian  black  hawaiian  hispanic  unknown  white  format_ol  \\\n",
       "0         0    0.0    0.0       0.0       0.0      0.0    1.0          0   \n",
       "1         1    0.0    0.0       0.0       0.0      0.0    1.0          0   \n",
       "4         1    0.0    0.0       0.0       0.0      0.0    1.0          1   \n",
       "5         0    1.0    0.0       0.0       0.0      0.0    0.0          1   \n",
       "7         1    1.0    0.0       0.0       0.0      0.0    0.0          0   \n",
       "..      ...    ...    ...       ...       ...      ...    ...        ...   \n",
       "316       0    0.0    0.0       0.0       0.0      0.0    1.0          0   \n",
       "317       1    NaN    NaN       NaN       NaN      NaN    NaN          0   \n",
       "319       1    NaN    NaN       NaN       NaN      NaN    NaN          1   \n",
       "320       0    NaN    NaN       NaN       NaN      NaN    NaN          1   \n",
       "321       1    NaN    NaN       NaN       NaN      NaN    NaN          1   \n",
       "\n",
       "     format_blended  falsexam  \n",
       "0               0.0  63.29997  \n",
       "1               0.0  79.96000  \n",
       "4               0.0  83.30000  \n",
       "5               0.0  88.34996  \n",
       "7               0.0  90.00000  \n",
       "..              ...       ...  \n",
       "316             0.0  91.65000  \n",
       "317             0.0  84.99000  \n",
       "319             0.0  70.05000  \n",
       "320             0.0  66.69000  \n",
       "321             0.0  83.29997  \n",
       "\n",
       "[214 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.OLS at 0x1c9b6ebaf90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('falsexam ~ format_ol', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>falsexam</td>     <th>  R-squared:         </th> <td>   0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   8.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 20 Feb 2024</td> <th>  Prob (F-statistic):</th>  <td>0.00382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:33:03</td>     <th>  Log-Likelihood:    </th> <td> -837.85</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   214</td>      <th>  AIC:               </th> <td>   1680.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   212</td>      <th>  BIC:               </th> <td>   1686.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   78.5475</td> <td>    1.113</td> <td>   70.563</td> <td> 0.000</td> <td>   76.353</td> <td>   80.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>format_ol</th> <td>   -4.9122</td> <td>    1.680</td> <td>   -2.925</td> <td> 0.004</td> <td>   -8.223</td> <td>   -1.601</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>129.000</td> <th>  Durbin-Watson:     </th> <td>   2.091</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1179.385</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.179</td>  <th>  Prob(JB):          </th> <td>7.94e-257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>13.643</td>  <th>  Cond. No.          </th> <td>    2.50</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &     falsexam     & \\textbf{  R-squared:         } &     0.039   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.034   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     8.554   \\\\\n",
       "\\textbf{Date:}             & Tue, 20 Feb 2024 & \\textbf{  Prob (F-statistic):} &  0.00382    \\\\\n",
       "\\textbf{Time:}             &     15:33:03     & \\textbf{  Log-Likelihood:    } &   -837.85   \\\\\n",
       "\\textbf{No. Observations:} &         214      & \\textbf{  AIC:               } &     1680.   \\\\\n",
       "\\textbf{Df Residuals:}     &         212      & \\textbf{  BIC:               } &     1686.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}  &      78.5475  &        1.113     &    70.563  &         0.000        &       76.353    &       80.742     \\\\\n",
       "\\textbf{format\\_ol} &      -4.9122  &        1.680     &    -2.925  &         0.004        &       -8.223    &       -1.601     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 129.000 & \\textbf{  Durbin-Watson:     } &     2.091  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  1179.385  \\\\\n",
       "\\textbf{Skew:}          &  -2.179 & \\textbf{  Prob(JB):          } & 7.94e-257  \\\\\n",
       "\\textbf{Kurtosis:}      &  13.643 & \\textbf{  Cond. No.          } &      2.50  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               falsexam   R-squared:                       0.039\n",
       "Model:                            OLS   Adj. R-squared:                  0.034\n",
       "Method:                 Least Squares   F-statistic:                     8.554\n",
       "Date:                Tue, 20 Feb 2024   Prob (F-statistic):            0.00382\n",
       "Time:                        15:33:03   Log-Likelihood:                -837.85\n",
       "No. Observations:                 214   AIC:                             1680.\n",
       "Df Residuals:                     212   BIC:                             1686.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     78.5475      1.113     70.563      0.000      76.353      80.742\n",
       "format_ol     -4.9122      1.680     -2.925      0.004      -8.223      -1.601\n",
       "==============================================================================\n",
       "Omnibus:                      129.000   Durbin-Watson:                   2.091\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1179.385\n",
       "Skew:                          -2.179   Prob(JB):                    7.94e-257\n",
       "Kurtosis:                      13.643   Cond. No.                         2.50\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('falsexam ~ format_ol', data=data).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<class 'statsmodels.iolib.table.SimpleTable'>,\n",
       " <class 'statsmodels.iolib.table.SimpleTable'>,\n",
       " <class 'statsmodels.iolib.table.SimpleTable'>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('falsexam ~ format_ol', data=data).fit().summary().tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   78.5475</td> <td>    1.113</td> <td>   70.563</td> <td> 0.000</td> <td>   76.353</td> <td>   80.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>format_ol</th> <td>   -4.9122</td> <td>    1.680</td> <td>   -2.925</td> <td> 0.004</td> <td>   -8.223</td> <td>   -1.601</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}  &      78.5475  &        1.113     &    70.563  &         0.000        &       76.353    &       80.742     \\\\\n",
       "\\textbf{format\\_ol} &      -4.9122  &        1.680     &    -2.925  &         0.004        &       -8.223    &       -1.601     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('falsexam ~ format_ol', data=data).fit().summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### End of Insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite amazing. We are not only able to estimate the ATE, but we also get, for free, confidence intervals and P-Values out of it! More than that, we can see that regression is doing exactly what it supposed to do: comparing $E[Y|T=0]$ and $E[Y|T=1]$. The intercept is exactly the sample mean when $T=0$, $E[Y|T=0]$, and the coefficient of the online format is exactly the sample difference in means $E[Y|T=1] - E[Y|T=0]$. Don't trust me? No problem. You can see for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "format_ol\n",
       "0    78.547485\n",
       "1    73.635263\n",
       "Name: falsexam, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data\n",
    " .groupby(\"format_ol\")\n",
    " [\"falsexam\"]\n",
    " .mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected. If you add to the intercept the ATE, that is, the parameter estimate of online format, you get the sample mean for the treated: $78.5475 + (-4.9122) = 73.635263$.\n",
    "\n",
    "## Regression Theory\n",
    "\n",
    "I don't intend to dive too deep into how linear regression is constructed and estimated. However, a little bit of theory will go a long way in explaining its power in causal inference. First of all, regression solves a theoretical best linear prediction problem. Let $\\beta^*$ be a vector of parameters:\n",
    "\n",
    "$\n",
    "\\beta^* =\\underset{\\beta}{argmin} \\ E[(Y_i - X_i'\\beta)^2]\n",
    "$\n",
    "\n",
    "Linear regression finds the parameters that minimise the mean squared error (MSE). \n",
    "\n",
    "If you differentiate it and set it to zero, you will find that the linear solution to this problem is given by\n",
    "\n",
    "$\n",
    "\\beta^* = E[X_i'X_i]^{-1}E[X_i' Y_i]\n",
    "$\n",
    "\n",
    "We can estimate this beta using the sample equivalent:\n",
    "\n",
    "$\n",
    "\\hat{\\beta} = (X'X)^{-1}X' Y\n",
    "$\n",
    "\n",
    "But don't take my word for it. If you are one of those that understand code better than formulas, try for yourself:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Insert by dataqubit\n",
    "\n",
    "> By GPT-4:\n",
    "\n",
    "### Starting with the MSE\n",
    "\n",
    "The MSE is expressed as the sum of squared differences between the actual values $Y$ and the predicted values $X\\beta$:\n",
    "\n",
    "$$ \\text{MSE} = (Y - X\\beta)'(Y - X\\beta) $$\n",
    "\n",
    "### Differentiating the MSE\n",
    "\n",
    "To minimize the MSE, we take its derivative with respect to $\\beta$. The derivative of this quadratic form can be expanded using matrix calculus rules.\n",
    "\n",
    "#### Expansion of the Derivative\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left( (Y - X\\beta)'(Y - X\\beta) \\right) $$\n",
    "\n",
    "This expands to:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left( Y'Y - Y'X\\beta - \\beta'X'Y + \\beta'X'X\\beta \\right) $$\n",
    "\n",
    "Let's differentiate each term separately:\n",
    "\n",
    "1. **Term 1: $Y'Y$**\n",
    "   - This term is a constant with respect to $\\beta$, so its derivative is 0.\n",
    "\n",
    "2. **Term 2: $-Y'X\\beta$**\n",
    "   - This is a linear term in $\\beta$. Its derivative is simply $-X'Y$ (note that we transpose the product due to the rules of matrix differentiation).\n",
    "\n",
    "3. **Term 3: $-\\beta'X'Y$**\n",
    "   - Similar to Term 2, this is also linear in $\\beta$, and its derivative is $-X'Y$.\n",
    "\n",
    "4. **Term 4: $\\beta'X'X\\beta$**\n",
    "   - This is a quadratic term in $\\beta$. Its derivative, by matrix differentiation rules, is $2X'X\\beta$.\n",
    "\n",
    "#### Combining the Terms\n",
    "\n",
    "Combining these, the derivative of the MSE with respect to $\\beta$ is:\n",
    "\n",
    "$$ -2X'Y + 2X'X\\beta $$\n",
    "\n",
    "### Setting the Derivative to Zero\n",
    "\n",
    "To find the value of $\\beta$ that minimizes the MSE, we set this derivative to zero:\n",
    "\n",
    "$$ -2X'Y + 2X'X\\beta = 0 $$\n",
    "\n",
    "Rearranging, we obtain the Normal Equation:\n",
    "\n",
    "$$ X'X\\beta = X'Y $$\n",
    "\n",
    "This is the key equation used to solve for the parameter vector $\\beta$ in linear regression. It shows that the optimal $\\beta$ is obtained when the derivative of the MSE is zero, leading us to a system of linear equations that can be solved using matrix operations, specifically by applying the matrix inverse of $X'X$ (provided it is invertible).\n",
    "\n",
    "\n",
    "> ### End of Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.9122215 , 78.54748458])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[[\"format_ol\"]].assign(intercep=1)\n",
    "y = data[\"falsexam\"]\n",
    "\n",
    "def regress(y, X): \n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
    "\n",
    "beta = regress(y, X)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulas above are pretty general. However, it pays off to study the case where we only have one regressor. In causal inference, we often want to estimate the causal impact of a variable $T$ on an outcome $y$. So, we use regression with this single variable to estimate this effect. Even if we include other variables in the model, they are usually just auxiliary. Adding other variables can help us estimate the causal effect of the treatment, but we are not very interested in estimating their parameters. \n",
    "\n",
    "With a single regressor variable $T$, the parameter associated to it will be given by\n",
    "\n",
    "$\n",
    "\\beta_1 = \\dfrac{Cov(Y_i, T_i)}{Var(T_i)} \n",
    "$\n",
    "\n",
    "If $T$ is randomly assigned, $\\beta_1$ is the ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.91222149822695"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kapa = data[\"falsexam\"].cov(data[\"format_ol\"]) / data[\"format_ol\"].var()\n",
    "kapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more than one regressor, we can extend the following formula to accommodate that. Let's say those other variables are just auxiliary and that we are truly interested only in estimating the parameter $\\kappa$ associated to $T$.\n",
    "\n",
    "$\n",
    "y_i = \\beta_0 + \\kappa T_i + \\beta_1 X_{1i} + ... +\\beta_k X_{ki} + u_i\n",
    "$\n",
    "\n",
    "$\\kappa$ can be obtained with the following formula\n",
    "\n",
    "$\n",
    "\\kappa = \\dfrac{Cov(Y_i, \\tilde{T_i})}{Var(\\tilde{T_i})} \n",
    "$\n",
    "\n",
    "> Insert by dataqubit   \n",
    "The formula above can be derived by taking covariates of both sides of equation with residual $\\tilde{T_i}$, and taking into account that covariate of the residual and $X$ matrices are zero by definition of the residual.\n",
    "> End of insert\n",
    "\n",
    "where $\\tilde{T_i}$ is the residual from a regression of $T_i$ on all other covariates $X_{1i}, ..., X_{ki}$. Now, let's appreciate how cool this is. It means that the coefficient of a multivariate regression is the bivariate coefficient of the same regressor **after accounting for the effect of other variables in the model**. In causal inference terms, $\\kappa$ is the bivariate coefficient of $T$ after having used all other variables to predict it.\n",
    "\n",
    "This has a nice intuition behind it. If we can predict $T$ using other variables, it means it's not random. However, we can make it so that $T$ is as good as random once we control for other available variables. To do so, we use linear regression to predict it from the other variables and then we take the residuals of that regression $\\tilde{T}$. By definition, $\\tilde{T}$ cannot be predicted by the other variables $X$ that we've already used to predict $T$. Quite elegantly, $\\tilde{T}$ is a version of the treatment that is not associated with any other variable in $X$.\n",
    "\n",
    "By the way, this is also a property of linear regression. The residual are always orthogonal or uncorrelated with any of the variables in the model that created it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonality imply that the dot product is zero: [7.81597009e-13 4.63984406e-12]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_ol</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>format_ol</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-9.419033e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>-9.419033e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              format_ol             e\n",
       "format_ol  1.000000e+00 -9.419033e-16\n",
       "e         -9.419033e-16  1.000000e+00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = y - X.dot(beta)\n",
    "print(\"Orthogonality imply that the dot product is zero:\", np.dot(e, X))\n",
    "X[[\"format_ol\"]].assign(e=e).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what is even cooler is that these properties don't depend on anything! They are mathematical truths, regardless of what your data looks like.\n",
    "\n",
    "## Regression For Non-Random Data\n",
    "\n",
    "So far, we worked with random experiment data but, as we know, those are hard to come by. Experiments are very expensive to conduct or simply infeasible. It's very hard to convince McKinsey & Co. to randomly provide their services free of charge so that we can, once and for all, distinguish the value their consulting services brings from the fact that those firms that can afford to pay them are already very well off.\n",
    "\n",
    "\n",
    "For this reason, we shall now delve into non random or observational data. In the following example, we will try to estimate the impact of an additional year of education on hourly wage. As you might have guessed, it is extremely hard to conduct an experiment with education. You can't simply randomize people to 4, 8 or 12 years of education. In this case observational data is all we have.\n",
    "\n",
    "First, let's estimate a very simple model. We will regress log hourly wages on years of education. We use logs here so that our parameter estimates have a percentage interpretation (if you never heard about this amazing properties of the log and want to know why that is, check out [this link](https://stats.stackexchange.com/questions/244199/why-is-it-that-natural-log-changes-are-percentage-changes-what-is-about-logs-th)). With it, we will be able to say that 1 extra year of education yields a wage increase of x%. \n",
    "\n",
    "\n",
    "$\n",
    "log(hwage)_i = \\beta_0 + \\beta_1 educ_i + u_i\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.3071</td> <td>    0.104</td> <td>   22.089</td> <td> 0.000</td> <td>    2.102</td> <td>    2.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>      <td>    0.0536</td> <td>    0.008</td> <td>    7.114</td> <td> 0.000</td> <td>    0.039</td> <td>    0.068</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wage = pd.read_csv(\"./data/wage.csv\").dropna()\n",
    "model_1 = smf.ols('np.log(hwage) ~ educ', data=wage.assign(hwage=wage[\"wage\"]/wage[\"hours\"])).fit()\n",
    "model_1.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate of $\\beta_1$ is 0.0536, with a 95% confidence interval of (0.039, 0.068). This means that this model predicts that wages will increase about 5.3% for every additional year of education. This percentage increase is inline with the belief that education impacts wages in an exponential fashion: we expect that going from 11 to 12 years of education (average to graduate high school) to be less rewarding than going from 14 to 16 years (average to graduate college)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"fivethirtyeight\")\n",
    "\n",
    "x = np.array(range(5, 20))\n",
    "plt.plot(x, np.exp(model_1.params[\"Intercept\"] + model_1.params[\"educ\"] * x))\n",
    "plt.xlabel(\"Years of Education\")\n",
    "plt.ylabel(\"Hourly Wage\")\n",
    "plt.title(\"Impact of Education on Hourly Wage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it is not because we can estimate this simple model that it's correct. Notice how I was carefully with my words saying it **predicts** wage from education. I never said that this prediction was causal. In fact, by now, you probably have very serious reasons to believe this model is biased. Since our data didn't come from a random experiment, we don't know if those that got more education are comparable to those who got less. Going even further, from our understanding of how the world works, we are very certain that they are not comparable. Namely, we can argue that those with more years of education probably have richer parents, and that the increase we are seeing in wages as we increase education is just a reflection of how the family wealth is associated with more years of education. Putting it in math terms, we think that $E[Y_0|T=0] < E[Y_0|T=1]$, that is, those with more education would have higher income anyway, even without so many years of education. If you are really grim about education, you can argue that it can even *reduce* wages by keeping people out of the workforce and lowering their experience.\n",
    "\n",
    "Fortunately, in our data, we have access to lots of other variables. We can see the parents' education `meduc`, `feduc`, the `IQ` score for that person, the number of years of experience `exper` and the tenure of the person in his or her current company `tenure`. We even have some dummy variables for marriage and black ethnicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>hours</th>\n",
       "      <th>lhwage</th>\n",
       "      <th>IQ</th>\n",
       "      <th>educ</th>\n",
       "      <th>exper</th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>married</th>\n",
       "      <th>black</th>\n",
       "      <th>south</th>\n",
       "      <th>urban</th>\n",
       "      <th>sibs</th>\n",
       "      <th>brthord</th>\n",
       "      <th>meduc</th>\n",
       "      <th>feduc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>769</td>\n",
       "      <td>40</td>\n",
       "      <td>2.956212</td>\n",
       "      <td>93</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>825</td>\n",
       "      <td>40</td>\n",
       "      <td>3.026504</td>\n",
       "      <td>108</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>650</td>\n",
       "      <td>40</td>\n",
       "      <td>2.788093</td>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>562</td>\n",
       "      <td>40</td>\n",
       "      <td>2.642622</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>91</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage  hours    lhwage   IQ  educ  exper  tenure  age  married  black  \\\n",
       "0   769     40  2.956212   93    12     11       2   31        1      0   \n",
       "2   825     40  3.026504  108    14     11       9   33        1      0   \n",
       "3   650     40  2.788093   96    12     13       7   32        1      0   \n",
       "4   562     40  2.642622   74    11     14       5   34        1      0   \n",
       "6   600     40  2.708050   91    10     13       0   30        0      0   \n",
       "\n",
       "   south  urban  sibs  brthord  meduc  feduc  \n",
       "0      0      1     1      2.0    8.0    8.0  \n",
       "2      0      1     1      2.0   14.0   14.0  \n",
       "3      0      1     4      3.0   12.0   12.0  \n",
       "4      0      1    10      6.0    6.0   11.0  \n",
       "6      0      1     1      2.0    8.0    8.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include all those extra variables in a model and estimate it:\n",
    "\n",
    "$\n",
    "log(hwage)_i = \\beta_0 + \\kappa \\ educ_i + \\pmb{\\beta}X_i + u_i\n",
    "$\n",
    "\n",
    "To understand how this helps with the bias problem, let's recap the bivariate breakdown of multivariate linear regression.\n",
    "\n",
    "$\n",
    "\\kappa = \\dfrac{Cov(Y_i, \\tilde{T_i})}{Var(\\tilde{T_i})} \n",
    "$\n",
    "\n",
    "This formula says that we can predict `educ` from the parents' education, from IQ, from experience and so on. After we do that, we'll be left with a version of `educ`, $\\tilde{educ}$, which is uncorrelated with all the variables included previously. This will break down arguments such as \"people that have more years of education have it because they have higher IQ. It is not the case that education leads to higher wages. It is just the case that it is correlated with IQ, which is what drives wages\". Well, if we include IQ in our model, then $\\kappa$ becomes the return of an additional year of education while keeping IQ fixed. Pause a little bit to understand what this implies. Even if we can't use randomised controlled trials to keep other factors equal between treated and untreated, regression can do this by including those same factors in the model, even if the data is not random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041147191010057635"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controls = ['IQ', 'exper', 'tenure', 'age', 'married', 'black',\n",
    "            'south', 'urban', 'sibs', 'brthord', 'meduc', 'feduc']\n",
    "\n",
    "X = wage[controls].assign(intercep=1)\n",
    "t = wage[\"educ\"]\n",
    "y = wage[\"lhwage\"]\n",
    "\n",
    "beta_aux = regress(t, X)\n",
    "t_tilde = t - X.dot(beta_aux)\n",
    "\n",
    "kappa = t_tilde.cov(y) / t_tilde.var()\n",
    "kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coefficient we've just estimated tells us that, for people with the same IQ, experience, tenure, age and so on, we should expect an additional year of education to be associated with a 4.11% increase in hourly wage. This confirms our suspicion that the first simple model with only `educ` was biased. It also confirms that this bias was overestimating the impact of education. Once we controlled for other factors, the estimated impact of education fell.\n",
    "\n",
    "If we are wiser and use software that other people wrote instead of coding everything yourself, we can even place a confidence interval around this estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.1156</td> <td>    0.232</td> <td>    4.802</td> <td> 0.000</td> <td>    0.659</td> <td>    1.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>      <td>    0.0411</td> <td>    0.010</td> <td>    4.075</td> <td> 0.000</td> <td>    0.021</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IQ</th>        <td>    0.0038</td> <td>    0.001</td> <td>    2.794</td> <td> 0.005</td> <td>    0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>exper</th>     <td>    0.0153</td> <td>    0.005</td> <td>    3.032</td> <td> 0.003</td> <td>    0.005</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tenure</th>    <td>    0.0094</td> <td>    0.003</td> <td>    2.836</td> <td> 0.005</td> <td>    0.003</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0086</td> <td>    0.006</td> <td>    1.364</td> <td> 0.173</td> <td>   -0.004</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>married</th>   <td>    0.1795</td> <td>    0.053</td> <td>    3.415</td> <td> 0.001</td> <td>    0.076</td> <td>    0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>black</th>     <td>   -0.0801</td> <td>    0.063</td> <td>   -1.263</td> <td> 0.207</td> <td>   -0.205</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>south</th>     <td>   -0.0397</td> <td>    0.035</td> <td>   -1.129</td> <td> 0.259</td> <td>   -0.109</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>urban</th>     <td>    0.1926</td> <td>    0.036</td> <td>    5.418</td> <td> 0.000</td> <td>    0.123</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sibs</th>      <td>    0.0065</td> <td>    0.009</td> <td>    0.722</td> <td> 0.470</td> <td>   -0.011</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brthord</th>   <td>   -0.0080</td> <td>    0.013</td> <td>   -0.604</td> <td> 0.546</td> <td>   -0.034</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>meduc</th>     <td>    0.0089</td> <td>    0.007</td> <td>    1.265</td> <td> 0.206</td> <td>   -0.005</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>feduc</th>     <td>    0.0069</td> <td>    0.006</td> <td>    1.113</td> <td> 0.266</td> <td>   -0.005</td> <td>    0.019</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = smf.ols('lhwage ~ educ +' + '+'.join(controls), data=wage).fit()\n",
    "model_2.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitted Variable or Confounding Bias\n",
    "\n",
    "The question that remains is: is this parameter we've estimated causal? Unfortunately, we can't say for sure. We can argue that the first simple model that regress wage on education probably isn't. It omits important variables that are correlated both with education and with wages. Without controlling for them, the estimated impact of education is also capturing the impact of those other variables that were not included in the model.\n",
    "\n",
    "To better understand how this bias work, let's suppose the true model for how education affects wage looks a bit like this\n",
    "\n",
    "$\n",
    "Wage_i = \\alpha + \\kappa \\ Educ_i + A_i'\\beta + u_i\n",
    "$\n",
    "\n",
    "wage is affected by education, which is measured by the size of $\\kappa$ and by additional ability factors, denoted as the vector $A$. If we omit ability from our model, our estimate for $\\kappa$ will look like this:\n",
    "\n",
    "$\n",
    "\\dfrac{Cov(Wage_i, Educ_i)}{Var(Educ_i)} = \\kappa + \\beta'\\delta_{A}\n",
    "$\n",
    "(`corrected`)\n",
    "\n",
    "where $\\delta_{A}$ is the vector of coefficients from the regression of $Educ$ on $A$\n",
    "\n",
    "The key point here is that it won't be exactly the $\\kappa$ that we want. Instead, it comes with this extra annoying term $\\beta'\\delta_{A}$. This term is the impact of the omitted $A$ on $Wage$, $\\beta$ times the impact of the omitted on the included $Educ$. This is important for economists that Joshua Angrist made a mantra out of it so that the students can recite it in meditation:\n",
    "\n",
    "```\n",
    "\"Short equals long \n",
    "plus the effect of omitted \n",
    "times the regression of omitted on included\"\n",
    "```\n",
    "\n",
    "Here, the short regression is the one that omits variables, while the long is the one that includes them. This formula or mantra gives us further insight into the nature of bias. First, the bias term will be zero if the omitted variables have no impact on the dependent variable $Y$. This makes total sense. I don't need to control for stuff that is irrelevant for wages when trying to understand the impact of education on it (like how tall the lilies of the field). Second, the bias term will also be zero if the omitted variables have no impact on the treatment variable. This also makes intuitive sense. If everything that impacts education has been included in the model, there is no way the estimated impact of education is mixed with a correlation from education on something else that also impacts wages. \n",
    "\n",
    "![img](data/img/linear-regression/confused_cat.png)\n",
    "\n",
    "To put it more succinctly, we say that **there is no OVB if all the confounding variables are accounted for in the model**. We can also leverage our knowledge about causal graphs here. A confounding variable is one that **causes both the treatment and the outcome**. In the wage example, IQ is a confounder. People with high IQ tend to complete more years of education because it's easier for them, so we can say that IQ causes education. People with high IQ also tend to be naturally more productive and consequently have higher wages, so IQ also causes wage. Since confounders are variables that affect both the treatment and the outcome, we mark them with an arrow going to T and Y. Here, I've denoted them with $W$. I've also marked positive causation with red and negative causation with blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"352pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 352.19 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184 348.19,-184 348.19,4 -4,4\"/>\n",
       "<!-- W -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>W</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"69\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"69\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">W</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- W&#45;&gt;T -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>W&#45;&gt;T</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.26,-144.76C54.28,-136.46 48.09,-126.15 42.48,-116.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"45.61,-115.22 37.47,-108.45 39.61,-118.82 45.61,-115.22\"/>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"62\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- W&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>W&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.14,-143.59C66.96,-119.61 64.82,-76.14 63.4,-47.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.91,-47.44 62.92,-37.62 59.91,-47.78 66.91,-47.44\"/>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.29,-72.41C39.3,-64.39 44.22,-54.57 48.73,-45.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"51.75,-47.33 53.09,-36.82 45.49,-44.2 51.75,-47.33\"/>\n",
       "</g>\n",
       "<!-- IQ -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>IQ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"161\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"161\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">IQ</text>\n",
       "</g>\n",
       "<!-- Educ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Educ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"149\" cy=\"-90\" rx=\"29.86\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"149\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Educ</text>\n",
       "</g>\n",
       "<!-- IQ&#45;&gt;Educ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>IQ&#45;&gt;Educ</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M158.03,-143.7C156.77,-136.32 155.26,-127.52 153.84,-119.25\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"157.33,-118.86 152.19,-109.6 150.43,-120.04 157.33,-118.86\"/>\n",
       "</g>\n",
       "<!-- Wage -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Wage</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"161\" cy=\"-18\" rx=\"32.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"161\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Wage</text>\n",
       "</g>\n",
       "<!-- IQ&#45;&gt;Wage -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>IQ&#45;&gt;Wage</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M171.63,-145.07C177.74,-134.93 184.82,-121.25 188,-108 191.73,-92.44 191.73,-87.56 188,-72 185.8,-62.82 181.72,-53.44 177.4,-45.17\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"180.6,-43.72 172.65,-36.71 174.49,-47.14 180.6,-43.72\"/>\n",
       "</g>\n",
       "<!-- Educ&#45;&gt;Wage -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Educ&#45;&gt;Wage</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M151.97,-71.7C153.23,-64.32 154.74,-55.52 156.16,-47.25\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"159.57,-48.04 157.81,-37.6 152.67,-46.86 159.57,-48.04\"/>\n",
       "</g>\n",
       "<!-- Crime -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>Crime</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300\" cy=\"-162\" rx=\"34.46\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Crime</text>\n",
       "</g>\n",
       "<!-- Police -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Police</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"269\" cy=\"-90\" rx=\"33.95\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"269\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Police</text>\n",
       "</g>\n",
       "<!-- Crime&#45;&gt;Police -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>Crime&#45;&gt;Police</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M292.5,-144.05C289.07,-136.32 284.92,-126.96 281.08,-118.27\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"284.3,-116.9 277.05,-109.17 277.9,-119.73 284.3,-116.9\"/>\n",
       "</g>\n",
       "<!-- Violence -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>Violence</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300\" cy=\"-18\" rx=\"44.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Violence</text>\n",
       "</g>\n",
       "<!-- Crime&#45;&gt;Violence -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>Crime&#45;&gt;Violence</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M304.87,-143.98C307.56,-133.67 310.64,-120.19 312,-108 313.78,-92.1 313.78,-87.9 312,-72 311.08,-63.81 309.4,-55.04 307.57,-47.04\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"311.01,-46.37 305.23,-37.49 304.21,-48.04 311.01,-46.37\"/>\n",
       "</g>\n",
       "<!-- Police&#45;&gt;Violence -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>Police&#45;&gt;Violence</title>\n",
       "<path fill=\"none\" stroke=\"blue\" d=\"M276.5,-72.05C279.93,-64.32 284.08,-54.96 287.92,-46.27\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"291.1,-47.73 291.95,-37.17 284.7,-44.9 291.1,-47.73\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1643864d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "\n",
    "g.edge(\"W\", \"T\"), g.edge(\"W\", \"Y\"), g.edge(\"T\", \"Y\")\n",
    "\n",
    "g.edge(\"IQ\", \"Educ\", color=\"red\"), g.edge(\"IQ\", \"Wage\", color=\"red\"), g.edge(\"Educ\", \"Wage\", color=\"red\")\n",
    "\n",
    "g.edge(\"Crime\", \"Police\", color=\"red\"), g.edge(\"Crime\", \"Violence\", color=\"red\"), \n",
    "g.edge(\"Police\", \"Violence\", color=\"blue\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal graphs are excellent to depict our understanding of the world and understand how confounding bias works. In our first example, we have a graph where education causes wage: more education leads to higher wages. However, IQ also causes wage and it also causes education: high IQ causes both more education and wage. If we don't account for IQ in our model, some of its effect on wage will flow through the correlation with education. That will make the impact of education look higher than it actually is. This is an example of positive bias.\n",
    "\n",
    "Just to give another example, but with negative bias, consider the causal graph about the effect of police on city violence. What we usually see in the world is that cities with higher police force also have more violence. Does this mean that the police are causing the violence? Well, it could be, I don't think it's worth getting into that discussion here. But, there is also a strong possibility that there is a confounding variable causing us to see a biased version of the impact of police on violence. It could be that increasing police force decreases violence. But, a third variable crime, causes both more violence and more police force. If we don't account for it, the impact of crime on violence will flow through police force, making it look like it increases violence. This is an example of negative bias.\n",
    "\n",
    "Causal graphs can also show us how both regression and randomized control trials are correct for confounding bias. RCT does so by severing the connection of the confounder to the treatment variable. By making $T$ random, by definition, nothing can cause it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"286pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 285.86 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 281.86,-112 281.86,4 -4,4\"/>\n",
       "<!-- W -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>W</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">W</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"80\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- W&#45;&gt;Y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>W&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M38.76,-73.46C45.42,-64.67 53.89,-53.48 61.43,-43.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.11,-45.78 67.36,-35.7 58.53,-41.56 64.11,-45.78\"/>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.4,-72.05C92.35,-64.49 89.87,-55.37 87.56,-46.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.95,-45.98 84.95,-37.25 84.2,-47.82 90.95,-45.98\"/>\n",
       "</g>\n",
       "<!-- IQ -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>IQ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"173\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">IQ</text>\n",
       "</g>\n",
       "<!-- Wage -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Wage</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"191\" cy=\"-18\" rx=\"32.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"191\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Wage</text>\n",
       "</g>\n",
       "<!-- IQ&#45;&gt;Wage -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>IQ&#45;&gt;Wage</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M177.36,-72.05C179.27,-64.6 181.58,-55.64 183.74,-47.22\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"187.08,-48.3 186.18,-37.74 180.3,-46.55 187.08,-48.3\"/>\n",
       "</g>\n",
       "<!-- Educ -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Educ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"248\" cy=\"-90\" rx=\"29.86\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"248\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Educ</text>\n",
       "</g>\n",
       "<!-- Educ&#45;&gt;Wage -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Educ&#45;&gt;Wage</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M235.35,-73.46C228.22,-64.71 219.15,-53.56 211.07,-43.64\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"213.99,-41.7 204.97,-36.15 208.57,-46.12 213.99,-41.7\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1643ccbb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "\n",
    "g.edge(\"W\", \"Y\"), g.edge(\"T\", \"Y\")\n",
    "\n",
    "g.edge(\"IQ\", \"Wage\", color=\"red\"), g.edge(\"Educ\", \"Wage\", color=\"red\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression, on the other hand, does so by comparing the effect of $T$ while maintaining the confounder $W$ set to a fixed level. With regression, it is not the case that W cease to cause T and Y. It is just that it is held fixed, so it can't influence changes on T and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"305pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 304.83 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 300.83,-112 300.83,4 -4,4\"/>\n",
       "<!-- W=w -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>W=w</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"32.41\" cy=\"-90\" rx=\"32.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"32.41\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">W=w</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"109.41\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.41\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"109.41\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.41\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M109.41,-71.7C109.41,-64.41 109.41,-55.73 109.41,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"112.91,-47.62 109.41,-37.62 105.91,-47.62 112.91,-47.62\"/>\n",
       "</g>\n",
       "<!-- IQ=x -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>IQ=x</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"185.41\" cy=\"-90\" rx=\"30.88\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.41\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">IQ=x</text>\n",
       "</g>\n",
       "<!-- Educ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Educ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"264.41\" cy=\"-90\" rx=\"29.86\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.41\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Educ</text>\n",
       "</g>\n",
       "<!-- Wage -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Wage</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"264.41\" cy=\"-18\" rx=\"32.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.41\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Wage</text>\n",
       "</g>\n",
       "<!-- Educ&#45;&gt;Wage -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Educ&#45;&gt;Wage</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M264.41,-71.7C264.41,-64.41 264.41,-55.73 264.41,-47.54\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"267.91,-47.62 264.41,-37.62 260.91,-47.62 267.91,-47.62\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x164384910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "\n",
    "g.node(\"W=w\"), g.edge(\"T\", \"Y\")\n",
    "g.node(\"IQ=x\"), g.edge(\"Educ\", \"Wage\", color=\"red\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to our question, is the parameter we've estimated for the impact of `educ` on wage causal? I'm sorry to bring it to you, but that will depend on our ability to argue in favor or against that fact that all confounders have been included in the model. Personally, I think they haven't. For instance, we haven't included family wealth. Even if we included family education, that can only be seen as a proxy for wealth. We've also not accounted for factors like personal ambition. It could be that ambition is what causes both more years of education and higher wage, so it is a confounder. This is to show that **causal inference with non-random or observational data should always be taken with a grain of salt**. We can never be sure that all confounders were accounted for.\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "We've covered a lot of ground with regression. We saw how regression can be used to perform A/B testing and how it conveniently gives us confidence intervals. Then, we moved to study how regression solves a prediction problem and it is the best linear approximation to the conditional expectation function (CEF). We've also discussed how, in the bivariate case, the regression treatment coefficient is the covariance between the treatment and the outcome divided by the variance of the treatment. Expanding to the multivariate case, we figured out how regression gives us a partialling out interpretation of the treatment coefficient: it can be interpreted as how the outcome would change with the treatment while keeping all other included variables constant. This is what economists love to refer as *ceteris paribus*.\n",
    "\n",
    "Finally, we took a turn to understanding bias. We saw how `Short equals long plus the effect of omitted times the regression of omitted on included`. This shed some light to how bias comes to be. We discovered that the source of omitted variable bias is confounding: a variable that affects both the treatment and the outcome. Lastly, we used causal graphs to see how RCT and regression fixes confounding.\n",
    "\n",
    "## References\n",
    "\n",
    "I like to think of this entire book as a tribute to Joshua Angrist, Alberto Abadie and Christopher Walters for their amazing Econometrics class. Most of the ideas here are taken from their classes at the American Economic Association. Watching them is what is keeping me sane during this tough year of 2020.\n",
    "* [Cross-Section Econometrics](https://www.aeaweb.org/conference/cont-ed/2017-webcasts)\n",
    "* [Mastering Mostly Harmless Econometrics](https://www.aeaweb.org/conference/cont-ed/2020-webcasts)\n",
    "\n",
    "I'll also like to reference the amazing books from Angrist. They have shown me that Econometrics, or 'Metrics as they call it, is not only extremely useful but also profoundly fun.\n",
    "\n",
    "* [Mostly Harmless Econometrics](https://www.mostlyharmlesseconometrics.com/)\n",
    "* [Mastering 'Metrics](https://www.masteringmetrics.com/)\n",
    "\n",
    "My final reference is Miguel Hernan and Jamie Robins' book. It has been my trustworthy companion in the most thorny causal questions I had to answer.\n",
    "\n",
    "* [Causal Inference Book](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)\n",
    "\n",
    "![img](./data/img/poetry.png)\n",
    "\n",
    "\n",
    "## Contribute\n",
    "\n",
    "Causal Inference for the Brave and True is an open-source material on causal inference, the statistics of science. It uses only free software, based in Python. Its goal is to be accessible monetarily and intellectually.\n",
    "If you found this book valuable and you want to support it, please go to [Patreon](https://www.patreon.com/causal_inference_for_the_brave_and_true). If you are not ready to contribute financially, you can also help by fixing typos, suggesting edits or giving feedback on passages you didn't understand. Just go to the book's repository and [open an issue](https://github.com/matheusfacure/python-causality-handbook/issues). Finally, if you liked this content, please share it with others who might find it useful and give it a [star on GitHub](https://github.com/matheusfacure/python-causality-handbook/stargazers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
